{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f2baff-b1c1-4796-9149-685afc1c7e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Import basic libraries\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import re\n",
    "\n",
    "# Make sure the stopwords exist\n",
    "\n",
    "_ = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad75a71-0bbd-4ff3-a731-eef452ffb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Routines\n",
    "#\n",
    "\n",
    "# Remove punctuations comes intermediate after a word\n",
    "\n",
    "def rm_postfix_punctuation(word:str):\n",
    "    # Create a copy for editing\n",
    "    word_tmp = word\n",
    "    \n",
    "    # Find locations of non-letter chars\n",
    "    for match in (list(re.finditer(r'[^\\w\\s]', word))[::-1]): \n",
    "        # Remove the last char if the last char is not a letter\n",
    "        if match.end() == len(word_tmp): \n",
    "            word_tmp = word_tmp[:len(word_tmp)-1] \n",
    "            \n",
    "    return word_tmp\n",
    "\n",
    "# Preprocessing for one tweet\n",
    "\n",
    "def preprocess_one_tweet(the_tweet:np.ndarray):\n",
    "    # Define a stemmer \n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    \n",
    "    # Split words from a tweet and store it in a list after preprocessing\n",
    "    tokens = [stemmer.stem(rm_postfix_punctuation(word)) \\\n",
    "             for word in the_tweet.lower().split() if \\\n",
    "             not (word.startswith(\"@\") or re.search(r'://', word) or\n",
    "                 word in nltk.corpus.stopwords.words(\"english\")) ]\n",
    "    \n",
    "    # Remove empty \"words\"\n",
    "    tokens = [word for word in tokens if len(word) > 0]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Create corpus and word count for a list of tweets\n",
    "\n",
    "def create_corpus(list_of_tweets:np.ndarray, size:int = 10000):\n",
    "    # Create an empty dict for word counting\n",
    "    word_count = dict()    \n",
    "    \n",
    "    for tweet in list_of_tweets:\n",
    "        # Get the tokens from the routine\n",
    "        tokens_tweet = preprocess_one_tweet(tweet) \n",
    "        \n",
    "        # Add one to the counter for each token\n",
    "        for word in tokens_tweet:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "            \n",
    "    # Sort the word count for the corpus\n",
    "    word_count = dict(sorted(word_count.items(), \n",
    "                             key=lambda x:x[1], reverse=True))\n",
    "    \n",
    "    # Create the corpus accordingly\n",
    "    corpus = dict([(x[1], x[0]+1) for x in enumerate(word_count.keys())][:(size-2)])\n",
    "    \n",
    "    # Create a special token for not included words\n",
    "    corpus[\":empty:\"] = 0\n",
    "    corpus[\":not_in_list:\"] = size-1\n",
    "    \n",
    "    # Sort the words for better presentation\n",
    "    corpus = dict(sorted(corpus.items(), \n",
    "                             key=lambda x:x[1], reverse=False))\n",
    "            \n",
    "    return corpus, word_count\n",
    "\n",
    "# Vectorize a sentance\n",
    "\n",
    "def vectorize_tweet(corpus:dict, tweet:str, max_tokens:int = 100):\n",
    "    # Get the tokens from the routine\n",
    "    tokens_tweet = preprocess_one_tweet(tweet) \n",
    "    \n",
    "    # Shorten the tweet accordingly\n",
    "    if len(tokens_tweet) > max_tokens:\n",
    "        tokens_tweet = tokens_tweet[:max_tokens]\n",
    "    \n",
    "    # Create empty list for the word vec\n",
    "    word_vec = list()\n",
    "    \n",
    "    # Getting the number of word according to corpus\n",
    "    for token in tokens_tweet:\n",
    "        word_vec.append(corpus.get(token, corpus[\":not_in_list:\"]))\n",
    "    \n",
    "    # If the tweet was too short, append 0s \n",
    "    if len(word_vec) < max_tokens:\n",
    "        word_vec = word_vec + [0] * (max_tokens - len(word_vec))\n",
    "        \n",
    "    return word_vec\n",
    "\n",
    "# Process the input x and output y\n",
    "\n",
    "def generate_x_and_y(data:np.ndarray, corpus:dict, max_tokens:int = 100):\n",
    "    # dict mapping labels to position in the one-hot vector\n",
    "    labels = dict([(x[1],x[0]) for x in enumerate(list(set(data[:,0])))])\n",
    "        \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    # for loop run through the data set\n",
    "    for idx in range(data.shape[0]):\n",
    "        # Create one-hot vector for desired output \n",
    "        y_new = [0] * len(labels)\n",
    "        y_new[labels[data[idx,0]]] = 1\n",
    "        \n",
    "        # Get the vectorized tweet as the input\n",
    "        x_new = vectorize_tweet(corpus, data[idx,1], max_tokens)\n",
    "        \n",
    "        y.append(y_new)\n",
    "        x.append(x_new)\n",
    "        \n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab46835-613c-41e7-83f7-2f967a75e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Obtain the dataset downloaded from Kaggle\n",
    "#\n",
    "\n",
    "original_dataset = pd.read_csv(\"datasets/Sentiment140_tweets.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196bbcab-5e2a-4c67-abd2-363686abc270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Showing first few lines of the dataset\n",
    "#\n",
    "\n",
    "original_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d98f65-3556-448d-961d-fb43c30f4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Handling the dataset and split it into different groups\n",
    "#\n",
    "\n",
    "# Split the datasets\n",
    "\n",
    "shuffled_rank = np.arange(original_dataset.values.data.shape[0])\n",
    "np.random.shuffle(shuffled_rank)\n",
    "shuffled_rank = shuffled_rank[:original_dataset.values.data.shape[0]//5]\n",
    "\n",
    "# Obtain the idx for different data-subsets\n",
    "\n",
    "idx4train = shuffled_rank[:int(len(shuffled_rank)*0.2)]\n",
    "idx4test = shuffled_rank[int(len(shuffled_rank)*0.2):]\n",
    "\n",
    "# Assign the datasets\n",
    "\n",
    "data_train = original_dataset.values[idx4train, 0:6:5]\n",
    "data_test = original_dataset.values[idx4test, 0:6:5]\n",
    "\n",
    "# Clean-up the original \n",
    "\n",
    "original_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d7a745-9831-4308-bd57-1b757bcb0c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0\n",
      " '@JoyceCamp thank god, yes i am. i just dented the hell out of the hood, and it left a turd on the headlights ']\n",
      "['thank', 'god', 'yes', 'am', 'dent', 'hell', 'hood', 'left', 'turd', 'headlight']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Show an example of one tweet\n",
    "#\n",
    "\n",
    "print(data_train[0])\n",
    "\n",
    "print(preprocess_one_tweet(data_train[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55048a4b-995c-4992-80d3-665f25540487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':empty:', 0), ('go', 1), (\"i'm\", 2), ('get', 3), ('day', 4), ('good', 5), ('work', 6), ('like', 7), ('love', 8), ('today', 9)]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Generate and check the corpus generated\n",
    "#\n",
    "\n",
    "corpus_train, word_count = create_corpus(data_train[:,1], 1000)\n",
    "\n",
    "print(list(corpus_train.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2380b2ab-19ef-460e-8974-12ef4c423d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Convert the data to vectors\n",
    "#\n",
    "\n",
    "x_train, y_train = generate_x_and_y(data_train, corpus_train, 20)\n",
    "x_test, y_test = generate_x_and_y(data_test, corpus_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b8d9ffb-d75c-4e9d-8361-0271cefea06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Free up the memory for my laptop\n",
    "#\n",
    "\n",
    "data_train = None\n",
    "data_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aae575ce-774f-4e18-bc1c-78f1cd2e182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         128000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 325,890\n",
      "Trainable params: 325,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Starting Design a Keras Model\n",
    "#\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Input for variable-length sequences of integers\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = keras.layers.Embedding(len(corpus_train), 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53373444-2831-4110-8656-35e6a8bb4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compile the model\n",
    "#\n",
    "\n",
    "model.compile(optimizer='adam',loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba4c464-c3a8-4bd1-bab2-a4b64086fbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2000/2000 [==============================] - 1076s 536ms/step - loss: 0.5723 - accuracy: 0.6875 - val_loss: 0.5311 - val_accuracy: 0.7314\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - 834s 417ms/step - loss: 0.5005 - accuracy: 0.7504 - val_loss: 0.5166 - val_accuracy: 0.7435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe7bc71adc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Training the model\n",
    "#\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=32, epochs=2, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248b66a-e41a-4cdc-b9ba-0549cda93286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
